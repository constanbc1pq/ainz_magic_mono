{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MagicArticulate API Server for Google Colab\n",
        "\n",
        "è¿™ä¸ª notebook å°†åœ¨ Colab ä¸­å¯åŠ¨ä¸€ä¸ª API æœåŠ¡å™¨ï¼Œä½¿ä½ çš„æœ¬åœ° ArticulateHub åç«¯èƒ½å¤Ÿä½¿ç”¨ GPU åŠ é€Ÿçš„ MagicArticulate æ¨¡å‹ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. æ£€æŸ¥ GPU å¯ç”¨æ€§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. å…‹éš† MagicArticulate ä»“åº“"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/your-repo/MagicArticulate.git\n",
        "%cd MagicArticulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. å®‰è£…ä¾èµ–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£… MagicArticulate ä¾èµ–\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# å®‰è£… API æœåŠ¡å™¨ä¾èµ–\n",
        "!pip install fastapi uvicorn pyngrok nest-asyncio python-multipart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. è®¾ç½® Ngrok è®¤è¯ï¼ˆå¯é€‰ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¦‚æœä½ æœ‰ ngrok è´¦å·ï¼Œå¯ä»¥è®¾ç½® authtoken ä»¥è·å¾—æ›´ç¨³å®šçš„è¿æ¥\n",
        "# ä» https://dashboard.ngrok.com/get-started/your-authtoken è·å–ä½ çš„ token\n",
        "# !ngrok authtoken YOUR_AUTH_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. åˆ›å»ºå¹¶å¯åŠ¨ API æœåŠ¡å™¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from fastapi import FastAPI, File, UploadFile, HTTPException, Form\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import FileResponse\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import shutil\n",
        "import tempfile\n",
        "import json\n",
        "\n",
        "# å…è®¸åœ¨ Jupyter ç¯å¢ƒä¸­è¿è¡Œ asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# æ·»åŠ  MagicArticulate åˆ° Python è·¯å¾„\n",
        "sys.path.append('/content/MagicArticulate')\n",
        "\n",
        "# å¯¼å…¥ MagicArticulateï¼ˆæ ¹æ®å®é™…çš„æ¨¡å—åè°ƒæ•´ï¼‰\n",
        "# from magicarticulate import MagicArticulate\n",
        "\n",
        "# åˆ›å»º FastAPI åº”ç”¨\n",
        "app = FastAPI(title=\"MagicArticulate API Server\")\n",
        "\n",
        "# é…ç½® CORS\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# åˆ›å»ºç»“æœç›®å½•\n",
        "RESULTS_DIR = \"/content/results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"MagicArticulate API Server is running on Colab!\"}\n",
        "\n",
        "@app.post(\"/process\")\n",
        "async def process_model(\n",
        "    file: UploadFile = File(...),\n",
        "    model_id: str = Form(...),\n",
        "    user_prompt: str = Form(...),\n",
        "    template_id: str = Form(...),\n",
        "    prompt_weight: float = Form(...)\n",
        "):\n",
        "    \"\"\"\n",
        "    å¤„ç†ä¸Šä¼ çš„3Dæ¨¡å‹\n",
        "    \"\"\"\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    try:\n",
        "        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶\n",
        "        input_path = os.path.join(temp_dir, file.filename)\n",
        "        with open(input_path, \"wb\") as f:\n",
        "            shutil.copyfileobj(file.file, f)\n",
        "        \n",
        "        # å‡†å¤‡è¾“å‡ºè·¯å¾„\n",
        "        output_filename = f\"{model_id}_articulated.obj\"\n",
        "        output_path = os.path.join(RESULTS_DIR, output_filename)\n",
        "        \n",
        "        print(f\"Processing model: {file.filename}\")\n",
        "        print(f\"Model ID: {model_id}\")\n",
        "        print(f\"User prompt: {user_prompt}\")\n",
        "        print(f\"Template: {template_id}, Weight: {prompt_weight}\")\n",
        "        \n",
        "        # TODO: åœ¨è¿™é‡Œè°ƒç”¨å®é™…çš„ MagicArticulate å¤„ç†\n",
        "        # result = magic_articulate.process(\n",
        "        #     input_path=input_path,\n",
        "        #     output_path=output_path,\n",
        "        #     prompt=user_prompt,\n",
        "        #     template=template_id,\n",
        "        #     weight=prompt_weight\n",
        "        # )\n",
        "        \n",
        "        # æ¨¡æ‹Ÿå¤„ç†ï¼ˆå®é™…ä½¿ç”¨æ—¶åˆ é™¤è¿™éƒ¨åˆ†ï¼‰\n",
        "        shutil.copy(input_path, output_path)\n",
        "        \n",
        "        return {\n",
        "            \"result_path\": output_path,\n",
        "            \"result_filename\": output_filename,\n",
        "            \"message\": \"Model processed successfully\"\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing model: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "    finally:\n",
        "        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
        "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
        "\n",
        "@app.get(\"/download/{filename}\")\n",
        "async def download_result(filename: str):\n",
        "    \"\"\"\n",
        "    ä¸‹è½½å¤„ç†ç»“æœ\n",
        "    \"\"\"\n",
        "    file_path = os.path.join(RESULTS_DIR, filename)\n",
        "    if not os.path.exists(file_path):\n",
        "        raise HTTPException(status_code=404, detail=\"File not found\")\n",
        "    \n",
        "    return FileResponse(file_path, filename=filename)\n",
        "\n",
        "@app.get(\"/status\")\n",
        "async def get_status():\n",
        "    \"\"\"\n",
        "    è·å–æœåŠ¡å™¨çŠ¶æ€\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"status\": \"running\",\n",
        "        \"gpu_available\": torch.cuda.is_available(),\n",
        "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None\n",
        "    }\n",
        "\n",
        "# å¯åŠ¨æœåŠ¡å™¨\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(f\"ğŸš€ API Server is running!\")\n",
        "print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "print(f\"\\nè¯·å°†è¿™ä¸ª URL é…ç½®åˆ°ä½ çš„æœ¬åœ° backend çš„ç¯å¢ƒå˜é‡ä¸­:\")\n",
        "print(f\"AI_SERVICE_URL={ngrok_tunnel.public_url}\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# å¯åŠ¨ FastAPI\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}