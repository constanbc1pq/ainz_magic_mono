{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MagicArticulate API Server for Google Colab\n",
        "\n",
        "这个 notebook 将在 Colab 中启动一个 API 服务器，使你的本地 ArticulateHub 后端能够使用 GPU 加速的 MagicArticulate 模型。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 检查 GPU 可用性"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 克隆 MagicArticulate 仓库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/your-repo/MagicArticulate.git\n",
        "%cd MagicArticulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 安装依赖"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 安装 MagicArticulate 依赖\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# 安装 API 服务器依赖\n",
        "!pip install fastapi uvicorn pyngrok nest-asyncio python-multipart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 设置 Ngrok 认证（可选）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 如果你有 ngrok 账号，可以设置 authtoken 以获得更稳定的连接\n",
        "# 从 https://dashboard.ngrok.com/get-started/your-authtoken 获取你的 token\n",
        "# !ngrok authtoken YOUR_AUTH_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 创建并启动 API 服务器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from fastapi import FastAPI, File, UploadFile, HTTPException, Form\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import FileResponse\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import shutil\n",
        "import tempfile\n",
        "import json\n",
        "\n",
        "# 允许在 Jupyter 环境中运行 asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# 添加 MagicArticulate 到 Python 路径\n",
        "sys.path.append('/content/MagicArticulate')\n",
        "\n",
        "# 导入 MagicArticulate（根据实际的模块名调整）\n",
        "# from magicarticulate import MagicArticulate\n",
        "\n",
        "# 创建 FastAPI 应用\n",
        "app = FastAPI(title=\"MagicArticulate API Server\")\n",
        "\n",
        "# 配置 CORS\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# 创建结果目录\n",
        "RESULTS_DIR = \"/content/results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"MagicArticulate API Server is running on Colab!\"}\n",
        "\n",
        "@app.post(\"/process\")\n",
        "async def process_model(\n",
        "    file: UploadFile = File(...),\n",
        "    model_id: str = Form(...),\n",
        "    user_prompt: str = Form(...),\n",
        "    template_id: str = Form(...),\n",
        "    prompt_weight: float = Form(...)\n",
        "):\n",
        "    \"\"\"\n",
        "    处理上传的3D模型\n",
        "    \"\"\"\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    try:\n",
        "        # 保存上传的文件\n",
        "        input_path = os.path.join(temp_dir, file.filename)\n",
        "        with open(input_path, \"wb\") as f:\n",
        "            shutil.copyfileobj(file.file, f)\n",
        "        \n",
        "        # 准备输出路径\n",
        "        output_filename = f\"{model_id}_articulated.obj\"\n",
        "        output_path = os.path.join(RESULTS_DIR, output_filename)\n",
        "        \n",
        "        print(f\"Processing model: {file.filename}\")\n",
        "        print(f\"Model ID: {model_id}\")\n",
        "        print(f\"User prompt: {user_prompt}\")\n",
        "        print(f\"Template: {template_id}, Weight: {prompt_weight}\")\n",
        "        \n",
        "        # TODO: 在这里调用实际的 MagicArticulate 处理\n",
        "        # result = magic_articulate.process(\n",
        "        #     input_path=input_path,\n",
        "        #     output_path=output_path,\n",
        "        #     prompt=user_prompt,\n",
        "        #     template=template_id,\n",
        "        #     weight=prompt_weight\n",
        "        # )\n",
        "        \n",
        "        # 模拟处理（实际使用时删除这部分）\n",
        "        shutil.copy(input_path, output_path)\n",
        "        \n",
        "        return {\n",
        "            \"result_path\": output_path,\n",
        "            \"result_filename\": output_filename,\n",
        "            \"message\": \"Model processed successfully\"\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing model: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "    finally:\n",
        "        # 清理临时文件\n",
        "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
        "\n",
        "@app.get(\"/download/{filename}\")\n",
        "async def download_result(filename: str):\n",
        "    \"\"\"\n",
        "    下载处理结果\n",
        "    \"\"\"\n",
        "    file_path = os.path.join(RESULTS_DIR, filename)\n",
        "    if not os.path.exists(file_path):\n",
        "        raise HTTPException(status_code=404, detail=\"File not found\")\n",
        "    \n",
        "    return FileResponse(file_path, filename=filename)\n",
        "\n",
        "@app.get(\"/status\")\n",
        "async def get_status():\n",
        "    \"\"\"\n",
        "    获取服务器状态\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"status\": \"running\",\n",
        "        \"gpu_available\": torch.cuda.is_available(),\n",
        "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None\n",
        "    }\n",
        "\n",
        "# 启动服务器\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(f\"🚀 API Server is running!\")\n",
        "print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "print(f\"\\n请将这个 URL 配置到你的本地 backend 的环境变量中:\")\n",
        "print(f\"AI_SERVICE_URL={ngrok_tunnel.public_url}\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# 启动 FastAPI\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}