"""
ç®€åŒ–ç‰ˆ Colab API æœåŠ¡å™¨
ç›´æ¥å¤åˆ¶ç²˜è´´åˆ° Colab å•å…ƒæ ¼ä¸­è¿è¡Œ
"""

# !pip install fastapi uvicorn pyngrok nest-asyncio python-multipart

import os
import shutil
import tempfile
from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
import uvicorn
import nest_asyncio
from pyngrok import ngrok

# å…è®¸åœ¨ Jupyter ç¯å¢ƒä¸­è¿è¡Œ asyncio
nest_asyncio.apply()

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(title="MagicArticulate Colab API")

# é…ç½® CORS - å…è®¸æ‰€æœ‰æ¥æº
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆ›å»ºç»“æœç›®å½•
UPLOAD_DIR = "/content/uploads"
RESULTS_DIR = "/content/results"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)

@app.get("/")
async def root():
    return {
        "message": "MagicArticulate API Server is running!",
        "endpoints": {
            "process": "POST /process",
            "download": "GET /download/{filename}",
            "status": "GET /status"
        }
    }

@app.post("/process")
async def process_model(
    file: UploadFile = File(...),
    model_id: str = Form(...),
    user_prompt: str = Form(...),
    template_id: str = Form(...),
    prompt_weight: float = Form(...)
):
    """
    å¤„ç†ä¸Šä¼ çš„3Dæ¨¡å‹
    """
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        input_filename = f"{model_id}_{file.filename}"
        input_path = os.path.join(UPLOAD_DIR, input_filename)
        
        with open(input_path, "wb") as f:
            content = await file.read()
            f.write(content)
        
        print(f"File saved: {input_path}")
        print(f"Processing with prompt: {user_prompt}")
        print(f"Template: {template_id}, Weight: {prompt_weight}")
        
        # TODO: åœ¨è¿™é‡Œè°ƒç”¨ MagicArticulate
        # ç¤ºä¾‹ä»£ç ï¼š
        # from magicarticulate import process_model
        # output_path = os.path.join(RESULTS_DIR, f"{model_id}_articulated.obj")
        # process_model(input_path, output_path, user_prompt, template_id, prompt_weight)
        
        # ä¸´æ—¶ï¼šç›´æ¥å¤åˆ¶æ–‡ä»¶ä½œä¸ºç»“æœï¼ˆå®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸º MagicArticulate å¤„ç†ï¼‰
        output_filename = f"{model_id}_articulated.obj"
        output_path = os.path.join(RESULTS_DIR, output_filename)
        shutil.copy(input_path, output_path)
        
        return {
            "result_path": output_path,
            "result_filename": output_filename,
            "message": "Model processed successfully"
        }
        
    except Exception as e:
        print(f"Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/download/{filename}")
async def download_result(filename: str):
    """
    ä¸‹è½½å¤„ç†ç»“æœ
    """
    file_path = os.path.join(RESULTS_DIR, filename)
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="File not found")
    
    return FileResponse(
        path=file_path,
        filename=filename,
        media_type='application/octet-stream'
    )

@app.get("/status")
async def get_status():
    """
    è·å–æœåŠ¡å™¨çŠ¶æ€
    """
    import torch
    return {
        "status": "running",
        "gpu_available": torch.cuda.is_available(),
        "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
        "upload_dir": UPLOAD_DIR,
        "results_dir": RESULTS_DIR
    }

# å¯åŠ¨æœåŠ¡å™¨
if __name__ == "__main__":
    # è®¾ç½® ngrok
    public_url = ngrok.connect(8000)
    print("\n" + "="*60)
    print("ğŸš€ MagicArticulate API Server is running!")
    print(f"ğŸ“¡ Public URL: {public_url}")
    print("\nğŸ”§ é…ç½®ä½ çš„æœ¬åœ° backend:")
    print(f"AI_SERVICE_URL={public_url}")
    print("="*60 + "\n")
    
    # è¿è¡ŒæœåŠ¡å™¨
    uvicorn.run(app, host="0.0.0.0", port=8000)